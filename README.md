# From PCA to VAE â€” Understanding Dimensionality Reduction

> How do we go from "project data onto a few important directions" to generating brand-new images from noise? I built this notebook to find out.

Everything runs on MNIST with a **2-D latent space**, so you can actually *see* what each method is doing â€” no abstract 128-dimensional hand-waving.

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gaurav-redhat/From-PCA-to-VAE/blob/main/dimensionality_reduction.ipynb)

<br>

## PCA â€” the linear baseline

The oldest trick in the book. Given high-dimensional data, PCA finds the directions of maximum variance and projects onto them.

```
         PCA
  x â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ z â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ xÌ‚
 (784-D)  V_k^T  (2-D)  V_k   (784-D)
         encode         decode
```

Given a centred data matrix **X âˆˆ â„^(nÃ—d)**:

- Compute SVD: **X = UÎ£V^T**
- Keep top-k right singular vectors: **V_k**
- Encode: **z = x Â· V_k**
- Decode: **xÌ‚ = z Â· V_k^T**

One matrix multiply in each direction â€” linear, closed-form, no training loop. The Eckartâ€“Young theorem guarantees this is the *best possible* rank-k approximation under MSE.

But "best linear" isn't great when the data has nonlinear structure. Two principal components can separate some digits, but the reconstructions are blurry smears and the clusters overlap:

<p align="center">
  <img src="images/pca_reconstruction.png" alt="PCA Reconstruction" width="700"/>
</p>

<p align="center">
  <img src="images/pca_latent_space.png" alt="PCA Latent Space" width="520"/>
</p>

Still â€” PCA gives us the core intuition that carries through everything else: **high-dimensional data often lives near a low-dimensional subspace**. The question is whether we can find a better one.

<br>

## Autoencoder â€” going nonlinear

Same compress-then-reconstruct idea, but now with neural networks so the mapping can bend.

```
         Encoder                    Decoder
  x â”€â”€â”€[784â†’512â†’256â†’128â†’2]â”€â”€â”€zâ”€â”€â”€[2â†’128â†’256â†’512â†’784]â”€â”€â”€â”€ xÌ‚
 (784-D)    f_Ï•(x)          (2-D)      g_Î¸(z)         (784-D)
```

We train by minimising reconstruction error:

**L = ||x - g_Î¸(f_Ï•(x))||Â²**

The nonlinearity lets it learn curved manifolds that PCA misses entirely. The improvement is immediate â€” reconstructions are sharper and the latent space clusters digits much more cleanly:

<p align="center">
  <img src="images/ae_reconstruction.png" alt="Autoencoder Reconstruction" width="700"/>
</p>

<p align="center">
  <img src="images/ae_latent_space.png" alt="Autoencoder Latent Space" width="520"/>
</p>

But there's a catch. The latent space has *no structure* â€” points are scattered wherever the network finds convenient. Large gaps between clusters are "dead zones." If you sample a random point and decode it, you'll likely get garbage.

This is a compressor, not a generator.

> *Fun fact:* a linear autoencoder (no activations) with MSE loss learns the exact same subspace as PCA.

<br>

## VAE â€” from compression to generation

The VAE is the main event. It's a proper generative model, and understanding *why* it's designed the way it is matters more than memorising the loss.

### The generative story

We imagine each image was generated by a two-step process:

```
  Step 1: Sample latent code     z ~ N(0, I)
  Step 2: Generate image         x ~ p_Î¸(x|z)    â† neural network decoder
```

The joint probability is easy to write down:

**p_Î¸(x, z) = p_Î¸(x|z) Â· p(z)**

To train this model, we want to maximise the marginal likelihood:

**p_Î¸(x) = âˆ« p_Î¸(x|z) Â· p(z) dz**

This integral sums over every possible z. Sounds fine in theory.

### Why this breaks

In practice, almost every z you sample from the prior produces an image nothing like the target x:

```
  zâ‚ ~ N(0,I)  â†’ decode â†’ ğŸ”²  â†’ p(x|zâ‚) â‰ˆ 0
  zâ‚‚ ~ N(0,I)  â†’ decode â†’ ğŸ”²  â†’ p(x|zâ‚‚) â‰ˆ 0
  zâ‚ƒ ~ N(0,I)  â†’ decode â†’ ğŸ”²  â†’ p(x|zâ‚ƒ) â‰ˆ 0
   ...
  z_K ~ N(0,I)  â†’ decode â†’ ğŸ”²  â†’ p(x|z_K) â‰ˆ 0

  Monte Carlo estimate: p(x) â‰ˆ (1/K) Î£ p(x|z_k) â‰ˆ 0   â† useless
```

The notebook actually runs this experiment. Even with 10,000 samples, the estimate is dominated by noise because no random z is "close enough" to the right one for a given x:

<p align="center">
  <img src="images/naive_monte_carlo.png" alt="Random prior samples produce images nothing like the target" width="700"/>
</p>
<p align="center"><i>Random z samples from the prior decode to noise â€” none match the target image, so p(x|z) â‰ˆ 0 for all of them.</i></p>

We can't compute p(x), so we can't compute the posterior p(z|x) = p(x|z)p(z)/p(x) either. We're stuck.

### The fix â€” ELBO

Instead of searching blindly, **learn an encoder** q_Ï•(z|x) that, given an image x, proposes z values that are actually useful for reconstructing it.

```
                    â”Œâ”€â”€â”€â”€ Î¼
  x â”€â”€[Encoder]â”€â”€â”¤              z = Î¼ + Ïƒ âŠ™ Îµ      Îµ ~ N(0,I)
                    â””â”€â”€â”€â”€ log ÏƒÂ²
                                    â”‚
                              [Decoder]
                                    â”‚
                                    xÌ‚
```

We can derive a lower bound on log p(x) â€” the **Evidence Lower Bound (ELBO)**:

```
  log p(x) â‰¥ E_q[log p(x|z)]  -  KL(q(z|x) || p(z))
              â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€       â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
              Reconstruction       Regularisation
              "decode well"        "stay close to N(0,I)"
```

**Derivation:**

```
  log p(x) = log âˆ« p(x,z) dz
           = log âˆ« [p(x,z)/q(z|x)] Â· q(z|x) dz
           â‰¥ âˆ« q(z|x) Â· log[p(x,z)/q(z|x)] dz        â† Jensen's inequality
           = E_q[log p(x|z)] - KL(q(z|x) || p(z))     â† split the log
```

The gap between log p(x) and the ELBO is exactly KL(q || p(z|x)) â‰¥ 0. As we tighten the bound, both the generative model and the encoder improve.

### Why this works (vs naive sampling)

| Naive Monte Carlo | ELBO |
|---|---|
| Samples z blindly from N(0,I) | Encoder proposes z tailored to each x |
| Almost all z give p(x\|z) â‰ˆ 0 | Encoder learns z that reconstructs x well |
| Estimate has infinite variance | Reparameterisation gives low-variance gradients |

### Reparameterisation trick

The encoder outputs Î¼ and log ÏƒÂ². To backpropagate through sampling:

**z = Î¼ + Ïƒ âŠ™ Îµ,  where Îµ ~ N(0, I)**

The randomness is in Îµ (independent of the encoder parameters), so z is a differentiable function of Ï•.

### KL divergence (closed form)

For diagonal Gaussian q vs standard Gaussian prior:

**KL = -Â½ Î£ (1 + log ÏƒÂ² - Î¼Â² - ÏƒÂ²)**

### Results

Because the KL term pushes the latent space toward N(0,I), the VAE latent space is smooth and well-structured. Reconstructions are solid, and the latent codes cluster by digit while staying close to a standard Gaussian:

<p align="center">
  <img src="images/vae_reconstruction.png" alt="VAE Reconstruction" width="700"/>
</p>

<p align="center">
  <img src="images/vae_latent_space.png" alt="VAE Latent Space" width="520"/>
</p>

And we can do something PCA and autoencoders can't â€” **generate brand-new images** by sampling z ~ N(0, I) and decoding:

<p align="center">
  <img src="images/vae_generated_digits.png" alt="VAE â€” Randomly Generated Digits" width="700"/>
</p>
<p align="center"><i>These digits never existed in the training set. Each one was sampled from noise.</i></p>

Sweeping zâ‚ and zâ‚‚ across the prior reveals the manifold structure â€” digits morph smoothly into each other with no dead zones:

<p align="center">
  <img src="images/vae_latent_manifold.png" alt="VAE Latent Space Manifold" width="550"/>
</p>
<p align="center"><i>Decoding a grid of z values from [-3, 3]Â². Nearby points produce similar digits â€” the space is continuous.</i></p>

<br>

## Putting it all together

All three methods squeeze data through a 2-D bottleneck and try to reconstruct it. The difference is what happens *inside* that bottleneck:

```
  Method          Encoder          Latent         Decoder         Can generate?
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  PCA             Linear (V_k^T)   Orthogonal     Linear (V_k)    No
  Autoencoder     Neural net       Unstructured   Neural net       Not really
  VAE             Neural net â†’ Î¼,Ïƒ Gaussian       Neural net       Yes
```

- **PCA** â€” rigid linear axes, globally optimal for MSE but can't bend
- **Autoencoder** â€” flexible but chaotic, good for compression, bad for generation
- **VAE** â€” flexible *and* structured, the KL term is the price we pay for a latent space we can actually sample from

The latent space comparison tells the whole story:

<p align="center">
  <img src="images/latent_space_comparison.png" alt="Latent Space Comparison â€” PCA vs Autoencoder vs VAE" width="800"/>
</p>
<p align="center"><i>Same 10,000 test digits projected into 2-D by each method. Colour = digit class.</i></p>

<br>

## Why this matters beyond MNIST

PCA's core insight â€” that high-dimensional data often lies near a low-rank subspace â€” shows up everywhere in modern ML. It directly motivates **LoRA** (Low-Rank Adaptation), where weight updates are approximated as **Î”W = BA** with B âˆˆ â„^(dÃ—r) and A âˆˆ â„^(rÃ—k), r â‰ª min(d, k) â€” the same rank-reduction idea, applied to fine-tuning large language models.

The trajectory from PCA â†’ Autoencoder â†’ VAE is really the story of learning better bottlenecks: from rigid linear projections, to flexible nonlinear ones, to structured probabilistic ones we can actually sample from.
