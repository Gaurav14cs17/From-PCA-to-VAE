{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dimensionality Reduction & Generative Models in PyTorch\n",
        "\n",
        "This notebook walks through three fundamental techniques for learning compact representations of data:\n",
        "\n",
        "| Method | Type | Latent Space | Generative? |\n",
        "|--------|------|-------------|-------------|\n",
        "| **PCA** | Linear projection | Orthogonal principal components | No |\n",
        "| **Autoencoder** | Nonlinear compression | Unconstrained bottleneck | Weak |\n",
        "| **VAE** | Probabilistic model | Regularised Gaussian latent | Yes |\n",
        "\n",
        "All three share a common thread: find a **low-dimensional representation** $\\mathbf{z}$ of high-dimensional data $\\mathbf{x}$, such that $\\mathbf{x}$ can be (approximately) recovered from $\\mathbf{z}$.\n",
        "\n",
        "We use **MNIST** throughout so results are directly comparable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Colab setup — installs are no-ops if packages already exist\n",
        "import importlib, subprocess, sys\n",
        "\n",
        "for pkg in [\"torch\", \"torchvision\", \"matplotlib\", \"numpy\"]:\n",
        "    if importlib.util.find_spec(pkg) is None:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
        "\n",
        "# Enable GPU: Runtime → Change runtime type → T4 GPU\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"Running on CPU — consider enabling a GPU runtime for faster training.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "train_dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
        "\n",
        "IMG_DIM = 28 * 28\n",
        "LATENT_DIM = 2  # 2-D latent for easy visualisation\n",
        "\n",
        "print(f\"Train: {len(train_dataset)} | Test: {len(test_dataset)} | Image dim: {IMG_DIM}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def show_images(originals, reconstructions, n=8, title=\"\"):\n",
        "    \"\"\"Side-by-side comparison of original vs reconstructed images.\"\"\"\n",
        "    fig, axes = plt.subplots(2, n, figsize=(n * 1.5, 3))\n",
        "    for i in range(n):\n",
        "        axes[0, i].imshow(originals[i].reshape(28, 28), cmap=\"gray\")\n",
        "        axes[0, i].axis(\"off\")\n",
        "        axes[1, i].imshow(reconstructions[i].reshape(28, 28), cmap=\"gray\")\n",
        "        axes[1, i].axis(\"off\")\n",
        "    axes[0, 0].set_title(\"Original\", fontsize=10)\n",
        "    axes[1, 0].set_title(\"Reconstructed\", fontsize=10)\n",
        "    fig.suptitle(title, fontsize=13, fontweight=\"bold\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_latent_space(z, labels, title=\"\"):\n",
        "    \"\"\"Scatter plot of 2-D latent codes coloured by digit class.\"\"\"\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    scatter = plt.scatter(z[:, 0], z[:, 1], c=labels, cmap=\"tab10\", s=2, alpha=0.6)\n",
        "    plt.colorbar(scatter, ticks=range(10))\n",
        "    plt.title(title, fontsize=14, fontweight=\"bold\")\n",
        "    plt.xlabel(\"$z_1$\")\n",
        "    plt.ylabel(\"$z_2$\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. PCA — Principal Component Analysis in PyTorch\n",
        "\n",
        "### Theory\n",
        "\n",
        "PCA finds an orthonormal basis $\\{\\mathbf{v}_1, \\dots, \\mathbf{v}_k\\}$ that maximises variance of the projected data.\n",
        "\n",
        "Given centred data matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$:\n",
        "\n",
        "$$\\mathbf{C} = \\frac{1}{n-1}\\mathbf{X}^\\top \\mathbf{X}$$\n",
        "\n",
        "The top-$k$ eigenvectors of $\\mathbf{C}$ (or equivalently, the right singular vectors from SVD of $\\mathbf{X}$) form the projection:\n",
        "\n",
        "$$\\mathbf{z} = \\mathbf{x} \\, \\mathbf{V}_k, \\qquad \\hat{\\mathbf{x}} = \\mathbf{z} \\, \\mathbf{V}_k^\\top$$\n",
        "\n",
        "### Key properties\n",
        "- **Linear** — the encoder and decoder are both matrix multiplications\n",
        "- **Globally optimal** under MSE for linear projections (Eckart–Young theorem)\n",
        "- **No training loop** — solved via eigendecomposition / SVD in one shot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class PCA:\n",
        "    \"\"\"PCA via SVD, implemented entirely in PyTorch.\"\"\"\n",
        "\n",
        "    def __init__(self, n_components: int):\n",
        "        self.k = n_components\n",
        "        self.mean = None\n",
        "        self.components = None  # (d, k)\n",
        "        self.singular_values = None\n",
        "        self.explained_variance_ratio = None\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def fit(self, X: torch.Tensor):\n",
        "        \"\"\"Fit PCA on data X of shape (n, d).\"\"\"\n",
        "        self.mean = X.mean(dim=0)\n",
        "        X_centered = X - self.mean\n",
        "\n",
        "        U, S, Vt = torch.linalg.svd(X_centered, full_matrices=False)\n",
        "\n",
        "        self.components = Vt[:self.k].T                # (d, k)\n",
        "        self.singular_values = S[:self.k]\n",
        "\n",
        "        total_var = (S ** 2).sum()\n",
        "        self.explained_variance_ratio = (S[:self.k] ** 2) / total_var\n",
        "        return self\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def transform(self, X: torch.Tensor) -> torch.Tensor:\n",
        "        return (X - self.mean) @ self.components\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def inverse_transform(self, Z: torch.Tensor) -> torch.Tensor:\n",
        "        return Z @ self.components.T + self.mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X_train = train_dataset.data.float().reshape(-1, IMG_DIM) / 255.0\n",
        "X_test = test_dataset.data.float().reshape(-1, IMG_DIM) / 255.0\n",
        "y_test = test_dataset.targets.numpy()\n",
        "\n",
        "pca = PCA(n_components=LATENT_DIM).fit(X_train)\n",
        "\n",
        "cumulative = pca.explained_variance_ratio.cumsum(0)\n",
        "print(f\"Explained variance (top {LATENT_DIM} components): {cumulative[-1]:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Variance explained vs number of components\n",
        "pca_full = PCA(n_components=50).fit(X_train)\n",
        "cumvar = pca_full.explained_variance_ratio.cumsum(0).numpy()\n",
        "\n",
        "plt.figure(figsize=(7, 4))\n",
        "plt.plot(range(1, 51), cumvar, \"o-\", markersize=4)\n",
        "plt.xlabel(\"Number of components\")\n",
        "plt.ylabel(\"Cumulative explained variance\")\n",
        "plt.title(\"PCA — Explained Variance\", fontweight=\"bold\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "z_pca = pca.transform(X_test)\n",
        "x_recon_pca = pca.inverse_transform(z_pca)\n",
        "\n",
        "pca_mse = F.mse_loss(x_recon_pca, X_test).item()\n",
        "print(f\"PCA reconstruction MSE: {pca_mse:.4f}\")\n",
        "\n",
        "show_images(X_test.numpy(), x_recon_pca.numpy(), title=\"PCA Reconstruction (2-D)\")\n",
        "plot_latent_space(z_pca.numpy(), y_test, title=\"PCA Latent Space (2-D)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. Autoencoder\n",
        "\n",
        "### Theory\n",
        "\n",
        "An autoencoder learns *nonlinear* encoder $f_\\phi$ and decoder $g_\\theta$ to minimise reconstruction error:\n",
        "\n",
        "$$\\mathcal{L}_{\\text{AE}} = \\|\\mathbf{x} - g_\\theta(f_\\phi(\\mathbf{x}))\\|^2$$\n",
        "\n",
        "With a bottleneck layer of dimension $k \\ll d$, the network is forced to learn a compressed representation.\n",
        "\n",
        "### PCA as a special case\n",
        "A **linear** autoencoder (no activations) with MSE loss learns the same subspace as PCA. The nonlinear activations let us capture more complex structure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, input_dim: int = IMG_DIM, latent_dim: int = LATENT_DIM):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, latent_dim),\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, input_dim),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        return self.encoder(x)\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encode(x)\n",
        "        return self.decode(z), z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def train_autoencoder(model, train_loader, epochs=20, lr=1e-3):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    model.train()\n",
        "    history = []\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        total_loss = 0.0\n",
        "        for batch, _ in train_loader:\n",
        "            x = batch.view(-1, IMG_DIM).to(device)\n",
        "            x_hat, _ = model(x)\n",
        "            loss = F.mse_loss(x_hat, x)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item() * x.size(0)\n",
        "\n",
        "        avg = total_loss / len(train_loader.dataset)\n",
        "        history.append(avg)\n",
        "        if epoch % 5 == 0 or epoch == 1:\n",
        "            print(f\"  Epoch {epoch:3d} | MSE: {avg:.4f}\")\n",
        "\n",
        "    return history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ae = Autoencoder().to(device)\n",
        "print(f\"Autoencoder parameters: {sum(p.numel() for p in ae.parameters()):,}\")\n",
        "print()\n",
        "ae_history = train_autoencoder(ae, train_loader, epochs=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.figure(figsize=(7, 4))\n",
        "plt.plot(ae_history, \"o-\", markersize=4)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"MSE Loss\")\n",
        "plt.title(\"Autoencoder Training\", fontweight=\"bold\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ae.eval()\n",
        "with torch.no_grad():\n",
        "    x_test_t = X_test.to(device)\n",
        "    x_recon_ae, z_ae = ae(x_test_t)\n",
        "    x_recon_ae = x_recon_ae.cpu().numpy()\n",
        "    z_ae = z_ae.cpu().numpy()\n",
        "\n",
        "ae_mse = np.mean((X_test.numpy() - x_recon_ae) ** 2)\n",
        "print(f\"Autoencoder reconstruction MSE: {ae_mse:.4f}\")\n",
        "\n",
        "show_images(X_test.numpy(), x_recon_ae, title=\"Autoencoder Reconstruction (2-D)\")\n",
        "plot_latent_space(z_ae, y_test, title=\"Autoencoder Latent Space (2-D)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. Variational Autoencoder (VAE)\n",
        "\n",
        "### 3.1 The goal: learn a generative model\n",
        "\n",
        "We want to learn a model that can **generate** new data like our training images.  \n",
        "The natural objective is **maximum likelihood** — find parameters $\\theta$ that maximise:\n",
        "\n",
        "$$p_\\theta(\\mathbf{x}) = \\prod_{i=1}^{N} p_\\theta(\\mathbf{x}^{(i)})$$\n",
        "\n",
        "or equivalently maximise $\\log p_\\theta(\\mathbf{x})$ for each data point.\n",
        "\n",
        "---\n",
        "\n",
        "### 3.2 Introducing latent variables — the joint probability\n",
        "\n",
        "We assume each image $\\mathbf{x}$ was generated from an unobserved latent code $\\mathbf{z}$:\n",
        "\n",
        "$$\\boxed{p_\\theta(\\mathbf{x}, \\mathbf{z}) = p_\\theta(\\mathbf{x}|\\mathbf{z}) \\; p(\\mathbf{z})}$$\n",
        "\n",
        "- **Prior:** $p(\\mathbf{z}) = \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ — simple Gaussian, easy to sample from\n",
        "- **Likelihood / decoder:** $p_\\theta(\\mathbf{x}|\\mathbf{z})$ — a neural network that maps $\\mathbf{z}$ to an image\n",
        "\n",
        "The **marginal likelihood** (the quantity we actually want to maximise) requires integrating out $\\mathbf{z}$:\n",
        "\n",
        "$$p_\\theta(\\mathbf{x}) = \\int p_\\theta(\\mathbf{x}|\\mathbf{z}) \\; p(\\mathbf{z}) \\; d\\mathbf{z}$$\n",
        "\n",
        "---\n",
        "\n",
        "### 3.3 The problem: why direct optimisation fails\n",
        "\n",
        "**Problem 1 — Intractable integral.**  \n",
        "The integral $\\int p_\\theta(\\mathbf{x}|\\mathbf{z}) \\, p(\\mathbf{z}) \\, d\\mathbf{z}$ sums over *every possible* $\\mathbf{z}$. When the decoder is a neural network this has no closed form. We could try Monte Carlo:\n",
        "\n",
        "$$p_\\theta(\\mathbf{x}) \\approx \\frac{1}{K}\\sum_{k=1}^{K} p_\\theta(\\mathbf{x}|\\mathbf{z}^{(k)}), \\quad \\mathbf{z}^{(k)} \\sim p(\\mathbf{z})$$\n",
        "\n",
        "but most random $\\mathbf{z}$ samples will produce images nothing like $\\mathbf{x}$, so $p_\\theta(\\mathbf{x}|\\mathbf{z}^{(k)}) \\approx 0$ for almost all samples — the estimate has **astronomically high variance**.\n",
        "\n",
        "**Problem 2 — Intractable posterior.**  \n",
        "If we could compute the true posterior $p_\\theta(\\mathbf{z}|\\mathbf{x}) = \\frac{p_\\theta(\\mathbf{x}|\\mathbf{z})\\,p(\\mathbf{z})}{p_\\theta(\\mathbf{x})}$ we could sample the \"right\" $\\mathbf{z}$ values. But computing it requires the very marginal $p_\\theta(\\mathbf{x})$ we can't evaluate.\n",
        "\n",
        "> **Bottom line:** We can write down $p_\\theta(\\mathbf{x}, \\mathbf{z})$ easily, but we can neither evaluate nor maximise $p_\\theta(\\mathbf{x})$ directly.\n",
        "\n",
        "Let's see this concretely below before introducing the fix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Demonstration: why naive Monte Carlo on the joint fails\n",
        "\n",
        "Below we build a simple decoder, pick a real image $\\mathbf{x}$, and estimate $p_\\theta(\\mathbf{x})$ by sampling $\\mathbf{z}$ from the prior.  Watch how the estimate is essentially **zero** — almost no random $\\mathbf{z}$ produces anything close to the target image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class SimpleDecoder(nn.Module):\n",
        "    \"\"\"A small decoder to demonstrate the intractability of the marginal.\"\"\"\n",
        "    def __init__(self, latent_dim=LATENT_DIM, output_dim=IMG_DIM):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 128), nn.ReLU(),\n",
        "            nn.Linear(128, 256), nn.ReLU(),\n",
        "            nn.Linear(256, output_dim), nn.Sigmoid(),\n",
        "        )\n",
        "    def forward(self, z):\n",
        "        return self.net(z)\n",
        "\n",
        "demo_decoder = SimpleDecoder().to(device)\n",
        "\n",
        "# Pick one real test image\n",
        "x_target = X_test[0:1].to(device)  # shape (1, 784)\n",
        "\n",
        "# --- Naive Monte Carlo: sample z ~ N(0,I), compute p(x|z) ---\n",
        "# Model p(x|z) as independent Bernoulli per pixel:\n",
        "#   log p(x|z) = sum_d [ x_d log mu_d + (1-x_d) log(1-mu_d) ]\n",
        "\n",
        "K_values = [10, 100, 1_000, 10_000]\n",
        "print(\"Naive Monte Carlo estimate of log p(x)  [sampling z from prior]\\n\")\n",
        "\n",
        "for K in K_values:\n",
        "    with torch.no_grad():\n",
        "        z_samples = torch.randn(K, LATENT_DIM).to(device)\n",
        "        mu = demo_decoder(z_samples)                             # (K, 784)\n",
        "        mu = mu.clamp(1e-6, 1 - 1e-6)\n",
        "        # log p(x|z) for each sample — sum over pixels\n",
        "        log_px_given_z = (x_target * mu.log() + (1 - x_target) * (1 - mu).log()).sum(dim=1)  # (K,)\n",
        "        # log p(x) ≈ log (1/K Σ exp(log p(x|z_k)))  via log-sum-exp\n",
        "        log_px_estimate = torch.logsumexp(log_px_given_z, dim=0) - np.log(K)\n",
        "    print(f\"  K = {K:>6,}  →  log p(x) ≈ {log_px_estimate.item():>10.1f}   \"\n",
        "          f\"(best single sample: {log_px_given_z.max().item():.1f})\")\n",
        "\n",
        "print(\"\\n→ Even with 10 000 samples the estimate is dominated by noise.\")\n",
        "print(\"  The true log p(x) should be around -100 to -200 for MNIST,\")\n",
        "print(\"  but almost every z from the prior produces garbage → p(x|z) ≈ 0.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Visualise: what does the prior produce vs the target?\n",
        "fig, axes = plt.subplots(2, 9, figsize=(14, 3.2))\n",
        "\n",
        "axes[0, 0].imshow(x_target.cpu().numpy().reshape(28, 28), cmap=\"gray\")\n",
        "axes[0, 0].set_title(\"Target x\", fontsize=9, fontweight=\"bold\")\n",
        "axes[0, 0].axis(\"off\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    z_rand = torch.randn(8, LATENT_DIM).to(device)\n",
        "    decoded_rand = demo_decoder(z_rand).cpu().numpy()\n",
        "\n",
        "for i in range(8):\n",
        "    axes[0, i + 1].imshow(decoded_rand[i].reshape(28, 28), cmap=\"gray\")\n",
        "    axes[0, i + 1].set_title(f\"z sample {i+1}\", fontsize=8)\n",
        "    axes[0, i + 1].axis(\"off\")\n",
        "\n",
        "# Bottom row: show the per-pixel log-likelihood — almost all near -∞\n",
        "with torch.no_grad():\n",
        "    mu_rand = demo_decoder(z_rand).clamp(1e-6, 1 - 1e-6)\n",
        "    ll = (x_target.cpu() * mu_rand.cpu().log() + (1 - x_target.cpu()) * (1 - mu_rand.cpu()).log()).sum(dim=1)\n",
        "\n",
        "axes[1, 0].axis(\"off\")\n",
        "for i in range(8):\n",
        "    axes[1, i + 1].text(0.5, 0.5, f\"log p(x|z)\\n{ll[i].item():.0f}\",\n",
        "                         ha=\"center\", va=\"center\", fontsize=9, transform=axes[1, i + 1].transAxes)\n",
        "    axes[1, i + 1].axis(\"off\")\n",
        "\n",
        "fig.suptitle(\"Random prior samples produce images nothing like x  →  p(x|z) ≈ 0\",\n",
        "             fontsize=12, fontweight=\"bold\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"This is WHY we need an encoder q(z|x) that proposes 'good' z values for each x.\")\n",
        "print(\"That insight leads us to the ELBO (section 3.4 above).\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.4 The solution: ELBO (Evidence Lower Bound)\n",
        "\n",
        "**Key idea:** introduce an *approximate* posterior $q_\\phi(\\mathbf{z}|\\mathbf{x})$ (an encoder network) that, given $\\mathbf{x}$, proposes **useful** $\\mathbf{z}$ values. Then derive a **lower bound** on $\\log p_\\theta(\\mathbf{x})$ that *is* tractable.\n",
        "\n",
        "**Derivation:**\n",
        "\n",
        "Start from the log marginal likelihood and multiply/divide by $q_\\phi$:\n",
        "\n",
        "$$\\log p_\\theta(\\mathbf{x}) = \\log \\int p_\\theta(\\mathbf{x}, \\mathbf{z}) \\, d\\mathbf{z} = \\log \\int \\frac{p_\\theta(\\mathbf{x}, \\mathbf{z})}{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\, q_\\phi(\\mathbf{z}|\\mathbf{x}) \\, d\\mathbf{z}$$\n",
        "\n",
        "Apply **Jensen's inequality** ($\\log$ is concave, so $\\log\\mathbb{E}[X] \\geq \\mathbb{E}[\\log X]$):\n",
        "\n",
        "$$\\log p_\\theta(\\mathbf{x}) \\geq \\int q_\\phi(\\mathbf{z}|\\mathbf{x}) \\log \\frac{p_\\theta(\\mathbf{x}, \\mathbf{z})}{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\, d\\mathbf{z} \\;=\\; \\mathcal{L}(\\theta, \\phi; \\mathbf{x})$$\n",
        "\n",
        "Expand the joint $p_\\theta(\\mathbf{x}, \\mathbf{z}) = p_\\theta(\\mathbf{x}|\\mathbf{z})\\,p(\\mathbf{z})$ and split the log:\n",
        "\n",
        "$$\\boxed{\\mathcal{L} = \\underbrace{\\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})}[\\log p_\\theta(\\mathbf{x}|\\mathbf{z})]}_{\\text{Reconstruction term}} \\;-\\; \\underbrace{D_{\\text{KL}}\\big(q_\\phi(\\mathbf{z}|\\mathbf{x}) \\,\\|\\, p(\\mathbf{z})\\big)}_{\\text{KL regularisation}}}$$\n",
        "\n",
        "**Why this fixes the problem:**\n",
        "\n",
        "| Naive Monte Carlo (Section 3.3) | ELBO |\n",
        "|---|---|\n",
        "| Samples $\\mathbf{z}$ blindly from prior $p(\\mathbf{z})$ | Samples $\\mathbf{z}$ from encoder $q_\\phi(\\mathbf{z}|\\mathbf{x})$ — tailored to each $\\mathbf{x}$ |\n",
        "| Almost all $\\mathbf{z}$ produce garbage for the given $\\mathbf{x}$ | Encoder learns to propose $\\mathbf{z}$ that reconstruct $\\mathbf{x}$ well |\n",
        "| Estimate has near-infinite variance | Low-variance gradient estimates via reparameterisation |\n",
        "\n",
        "**Interpretation of the two terms:**\n",
        "- **Reconstruction:** keep the decoder accurate — the encoder should propose $\\mathbf{z}$'s that let the decoder recover $\\mathbf{x}$\n",
        "- **KL:** keep the encoder close to the prior $\\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ — this is what gives the latent space its smooth, sampleable structure (prevents the encoder from collapsing each $\\mathbf{x}$ to a single point far from the prior)\n",
        "\n",
        "The gap between $\\log p_\\theta(\\mathbf{x})$ and the ELBO is exactly $D_{\\text{KL}}(q_\\phi \\| p_\\theta(\\mathbf{z}|\\mathbf{x})) \\geq 0$, so tightening the ELBO simultaneously improves both the generative model and the approximate posterior.\n",
        "\n",
        "---\n",
        "\n",
        "### 3.5 Reparameterisation trick\n",
        "\n",
        "We parameterise $q_\\phi(\\mathbf{z}|\\mathbf{x}) = \\mathcal{N}(\\boldsymbol{\\mu}_\\phi(\\mathbf{x}),\\; \\text{diag}(\\boldsymbol{\\sigma}_\\phi^2(\\mathbf{x})))$.\n",
        "\n",
        "To backpropagate through the stochastic sampling, we reparameterise:\n",
        "\n",
        "$$\\mathbf{z} = \\boldsymbol{\\mu} + \\boldsymbol{\\sigma} \\odot \\boldsymbol{\\epsilon}, \\quad \\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$$\n",
        "\n",
        "Now the randomness is in $\\boldsymbol{\\epsilon}$ (independent of $\\phi$), and $\\mathbf{z}$ is a deterministic, differentiable function of $\\phi$.\n",
        "\n",
        "### 3.6 KL divergence (closed form)\n",
        "\n",
        "For diagonal Gaussian $q$ vs standard Gaussian prior:\n",
        "\n",
        "$$D_{\\text{KL}} = -\\frac{1}{2}\\sum_{j=1}^{k}\\left(1 + \\log \\sigma_j^2 - \\mu_j^2 - \\sigma_j^2\\right)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.7 Putting it together — VAE implementation\n",
        "\n",
        "Now that we've seen:\n",
        "1. **Joint probability** $p_\\theta(\\mathbf{x}, \\mathbf{z}) = p_\\theta(\\mathbf{x}|\\mathbf{z})\\,p(\\mathbf{z})$ — easy to write down\n",
        "2. **Marginal** $p_\\theta(\\mathbf{x}) = \\int p_\\theta(\\mathbf{x}|\\mathbf{z})\\,p(\\mathbf{z})\\,d\\mathbf{z}$ — intractable to compute\n",
        "3. **Naive Monte Carlo** — almost all prior samples give $p(\\mathbf{x}|\\mathbf{z}) \\approx 0$\n",
        "4. **ELBO** — a tractable lower bound using an encoder $q_\\phi(\\mathbf{z}|\\mathbf{x})$\n",
        "\n",
        "We implement the full VAE below. The encoder outputs $\\boldsymbol{\\mu}$ and $\\log\\boldsymbol{\\sigma}^2$, we sample via the reparameterisation trick, decode, and optimise the ELBO."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, input_dim: int = IMG_DIM, latent_dim: int = LATENT_DIM):\n",
        "        super().__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        # Shared encoder backbone\n",
        "        self.encoder_backbone = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        # Two heads: mean and log-variance\n",
        "        self.fc_mu = nn.Linear(128, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(128, latent_dim)\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, input_dim),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder_backbone(x)\n",
        "        return self.fc_mu(h), self.fc_logvar(h)\n",
        "\n",
        "    def reparameterise(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + std * eps\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterise(mu, logvar)\n",
        "        return self.decode(z), mu, logvar, z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def vae_loss(x_hat, x, mu, logvar, beta=1.0):\n",
        "    \"\"\"ELBO = reconstruction (BCE) + beta * KL divergence.\"\"\"\n",
        "    recon = F.binary_cross_entropy(x_hat, x, reduction=\"sum\")\n",
        "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return recon + beta * kl, recon, kl\n",
        "\n",
        "\n",
        "def train_vae(model, train_loader, epochs=20, lr=1e-3, beta=1.0):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    model.train()\n",
        "    history = {\"total\": [], \"recon\": [], \"kl\": []}\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        t_total, t_recon, t_kl = 0.0, 0.0, 0.0\n",
        "        for batch, _ in train_loader:\n",
        "            x = batch.view(-1, IMG_DIM).to(device)\n",
        "            x_hat, mu, logvar, _ = model(x)\n",
        "            loss, recon, kl = vae_loss(x_hat, x, mu, logvar, beta)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            t_total += loss.item()\n",
        "            t_recon += recon.item()\n",
        "            t_kl += kl.item()\n",
        "\n",
        "        n = len(train_loader.dataset)\n",
        "        history[\"total\"].append(t_total / n)\n",
        "        history[\"recon\"].append(t_recon / n)\n",
        "        history[\"kl\"].append(t_kl / n)\n",
        "\n",
        "        if epoch % 5 == 0 or epoch == 1:\n",
        "            print(f\"  Epoch {epoch:3d} | ELBO: {t_total/n:.2f}  Recon: {t_recon/n:.2f}  KL: {t_kl/n:.2f}\")\n",
        "\n",
        "    return history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "vae = VAE().to(device)\n",
        "print(f\"VAE parameters: {sum(p.numel() for p in vae.parameters()):,}\")\n",
        "print()\n",
        "vae_history = train_vae(vae, train_loader, epochs=30, beta=1.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "for ax, key, color in zip(axes, [\"total\", \"recon\", \"kl\"], [\"#2196F3\", \"#4CAF50\", \"#FF9800\"]):\n",
        "    ax.plot(vae_history[key], \"o-\", markersize=3, color=color)\n",
        "    ax.set_xlabel(\"Epoch\")\n",
        "    ax.set_title(key.upper(), fontweight=\"bold\")\n",
        "    ax.grid(True, alpha=0.3)\n",
        "fig.suptitle(\"VAE Training Curves\", fontsize=14, fontweight=\"bold\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "vae.eval()\n",
        "with torch.no_grad():\n",
        "    x_test_t = X_test.to(device)\n",
        "    x_recon_vae, mu_vae, _, z_vae = vae(x_test_t)\n",
        "    x_recon_vae = x_recon_vae.cpu().numpy()\n",
        "    z_vae_np = mu_vae.cpu().numpy()  # use mean for deterministic plot\n",
        "\n",
        "vae_mse = np.mean((X_test.numpy() - x_recon_vae) ** 2)\n",
        "print(f\"VAE reconstruction MSE: {vae_mse:.4f}\")\n",
        "\n",
        "show_images(X_test.numpy(), x_recon_vae, title=\"VAE Reconstruction (2-D)\")\n",
        "plot_latent_space(z_vae_np, y_test, title=\"VAE Latent Space (2-D)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generating new digits\n",
        "\n",
        "Because the VAE latent space is regularised to match $\\mathcal{N}(\\mathbf{0}, \\mathbf{I})$, we can **sample** from it to generate new images that never existed in the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "vae.eval()\n",
        "with torch.no_grad():\n",
        "    # Sample from standard Gaussian\n",
        "    z_samples = torch.randn(16, LATENT_DIM).to(device)\n",
        "    generated = vae.decode(z_samples).cpu().numpy()\n",
        "\n",
        "fig, axes = plt.subplots(2, 8, figsize=(12, 3))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(generated[i].reshape(28, 28), cmap=\"gray\")\n",
        "    ax.axis(\"off\")\n",
        "fig.suptitle(\"VAE — Randomly Generated Digits\", fontsize=13, fontweight=\"bold\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 2-D manifold: sweep z1 and z2 across the prior to see smooth transitions\n",
        "n = 15\n",
        "grid = np.linspace(-3, 3, n)\n",
        "canvas = np.zeros((28 * n, 28 * n))\n",
        "\n",
        "vae.eval()\n",
        "with torch.no_grad():\n",
        "    for i, yi in enumerate(grid):\n",
        "        for j, xi in enumerate(grid):\n",
        "            z = torch.tensor([[xi, yi]], dtype=torch.float32).to(device)\n",
        "            img = vae.decode(z).cpu().numpy().reshape(28, 28)\n",
        "            canvas[i * 28:(i + 1) * 28, j * 28:(j + 1) * 28] = img\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(canvas, cmap=\"gray\")\n",
        "plt.title(\"VAE — Latent Space Manifold\", fontsize=14, fontweight=\"bold\")\n",
        "plt.xlabel(\"$z_1$\")\n",
        "plt.ylabel(\"$z_2$\")\n",
        "plt.xticks(np.linspace(0, 28 * n, 7), [f\"{v:.1f}\" for v in np.linspace(-3, 3, 7)])\n",
        "plt.yticks(np.linspace(0, 28 * n, 7), [f\"{v:.1f}\" for v in np.linspace(-3, 3, 7)])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. Side-by-Side Comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
        "\n",
        "for ax, z, title in zip(\n",
        "    axes,\n",
        "    [z_pca.numpy(), z_ae, z_vae_np],\n",
        "    [f\"PCA  (MSE={pca_mse:.4f})\",\n",
        "     f\"Autoencoder  (MSE={ae_mse:.4f})\",\n",
        "     f\"VAE  (MSE={vae_mse:.4f})\"],\n",
        "):\n",
        "    scatter = ax.scatter(z[:, 0], z[:, 1], c=y_test, cmap=\"tab10\", s=1, alpha=0.5)\n",
        "    ax.set_title(title, fontsize=12, fontweight=\"bold\")\n",
        "    ax.set_xlabel(\"$z_1$\")\n",
        "    ax.set_ylabel(\"$z_2$\")\n",
        "\n",
        "plt.colorbar(scatter, ax=axes[-1], ticks=range(10), label=\"Digit\")\n",
        "fig.suptitle(\"Latent Space Comparison (2-D)\", fontsize=15, fontweight=\"bold\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Reconstruction comparison on the same images\n",
        "idx = [0, 3, 5, 7, 11, 18, 42, 61]\n",
        "x_orig = X_test.numpy()[idx]\n",
        "\n",
        "fig, axes = plt.subplots(4, len(idx), figsize=(len(idx) * 1.5, 6))\n",
        "row_labels = [\"Original\", \"PCA\", \"Autoencoder\", \"VAE\"]\n",
        "recons = [x_orig, x_recon_pca.numpy()[idx], x_recon_ae[idx], x_recon_vae[idx]]\n",
        "\n",
        "for row, (imgs, label) in enumerate(zip(recons, row_labels)):\n",
        "    for col in range(len(idx)):\n",
        "        axes[row, col].imshow(imgs[col].reshape(28, 28), cmap=\"gray\")\n",
        "        axes[row, col].axis(\"off\")\n",
        "    axes[row, 0].set_ylabel(label, fontsize=11, fontweight=\"bold\", rotation=0, labelpad=70, va=\"center\")\n",
        "\n",
        "fig.suptitle(\"Reconstruction Comparison\", fontsize=14, fontweight=\"bold\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. Summary\n",
        "\n",
        "| | PCA | Autoencoder | VAE |\n",
        "|---|---|---|---|\n",
        "| **Mapping** | Linear | Nonlinear | Nonlinear + probabilistic |\n",
        "| **Training** | Closed-form (SVD) | Gradient descent | Gradient descent |\n",
        "| **Loss** | Reconstruction (MSE) | Reconstruction (MSE) | ELBO = Recon + KL |\n",
        "| **Latent structure** | Orthogonal axes | Unconstrained | Regularised Gaussian |\n",
        "| **Generation** | Not meaningful | Poor (holes in latent space) | Smooth sampling from $\\mathcal{N}(0, I)$ |\n",
        "| **Interpolation** | Linear only | May be discontinuous | Smooth and meaningful |\n",
        "\n",
        "### Key takeaways\n",
        "\n",
        "1. **PCA** gives the best *linear* compression. It's fast, parameter-free, and a great baseline.\n",
        "2. **Autoencoders** can learn richer representations via nonlinear mappings, but their latent space has no structure — you can't sample from it to generate realistic data.\n",
        "3. **VAEs** regularise the latent space to be a smooth Gaussian, enabling meaningful generation and interpolation at the cost of slightly blurrier reconstructions.\n",
        "\n",
        "### Connection to LoRA\n",
        "\n",
        "PCA's core insight — that high-dimensional data often lies near a *low-rank* subspace — directly motivates **LoRA**. \n",
        "LoRA approximates weight updates as $\\Delta W = BA$ where $B \\in \\mathbb{R}^{d \\times r}$ and $A \\in \\mathbb{R}^{r \\times k}$ with $r \\ll \\min(d, k)$, just as PCA projects through $\\mathbf{V}_k$ of rank $k \\ll d$."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}